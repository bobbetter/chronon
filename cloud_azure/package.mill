package build.cloud_azure
import mill.api._
import mill.scalalib._

// Cloud Azure module - supports both cloud_azure.test AND cloud_azure[2.13.17].test
object `package` extends Cross[CloudAzureModule](build.Constants.scalaVersions) with CloudAzureModule {
  // Provide default scala version for bracket-free access
  override val crossValue = build.Constants.defaultScalaVersion
}

trait CloudAzureModule extends Cross.Module[String] with build.BaseModule {
  def moduleDeps = Seq(build.spark(crossValue), build.aggregator(crossValue), build.api(crossValue), build.online(crossValue), build.redis(crossValue))

  def excludeJackson(dep: mill.scalalib.Dep): mill.scalalib.Dep = {
    dep
      .exclude("com.fasterxml.jackson.core" -> "jackson-core")
      .exclude("com.fasterxml.jackson.core" -> "jackson-databind")
      .exclude("com.fasterxml.jackson.core" -> "jackson-annotations")
      .exclude("com.fasterxml.jackson.datatype" -> "jackson-datatype-jsr310")
  }

  def mvnDeps = build.Constants.commonDeps ++ build.Constants.loggingApiDeps ++ build.Constants.utilityDeps ++ build.Constants.sparkDeps ++ Seq(
    // Azure Identity for authentication
    mvn"com.azure:azure-identity:1.14.2",
    // Azure Key Vault for secret management (e.g., Snowflake private keys)
    mvn"com.azure:azure-security-keyvault-secrets:4.10.4",
    mvn"net.snowflake:snowflake-jdbc:3.16.0",
    mvn"net.snowflake:spark-snowflake_2.12:3.1.6",
    // BouncyCastle for parsing PEM private keys (required by Snowflake JDBC for key pair auth)
    mvn"org.bouncycastle:bcprov-jdk18on:1.80",
    mvn"org.bouncycastle:bcpkix-jdk18on:1.80",
    // Hadoop Azure FileSystem connector (for abfss:// support)
    mvn"org.apache.iceberg:iceberg-azure-bundle:1.10.0",

    // Azure Cosmos DB SDK
    mvn"com.azure:azure-cosmos:4.76.0",
    // Azure Cosmos DB Spark Connector
    // mvn"com.azure.cosmos.spark:azure-cosmos-spark_3-5_2-12:4.42.0",
    // CLI argument parsing for Spark loader
    mvn"org.rogach::scallop:5.0.0",
  ).map(excludeJackson) ++ Seq(
    mvn"com.fasterxml.jackson.core:jackson-core:2.15.2",
    mvn"com.fasterxml.jackson.core:jackson-databind:2.15.2",
    mvn"com.fasterxml.jackson.core:jackson-annotations:2.15.2",
    mvn"com.fasterxml.jackson.module::jackson-module-scala:2.15.2",
    mvn"com.fasterxml.jackson.datatype:jackson-datatype-jsr310:2.15.2",
    // Reactor and Netty dependencies - force versions required by azure-core-http-netty 1.16.2
    // These versions are required to avoid NoSuchMethodError with reactor-netty
    // Let reactor-netty pull in the correct Netty versions transitively
    // See: https://central.sonatype.com/artifact/com.azure/azure-core-http-netty/1.16.2
    mvn"io.projectreactor:reactor-core:3.6.11",
    mvn"io.projectreactor.netty:reactor-netty-core:1.2.13",
    mvn"io.projectreactor.netty:reactor-netty-http:1.2.13",
  ).map(_.forceVersion())

  override def assemblyRules = super.assemblyRules ++ Seq(
    Assembly.Rule.Exclude("META-INF/versions/17/reactor/netty/resources/DefaultLoopNIO.class")
  )


  object test extends build.BaseTestModule {
    def scalaVersion = crossValue

    def moduleDeps = Seq(build.cloud_azure(crossValue), build.api(crossValue), build.online(crossValue), build.spark(crossValue).test, build.redis(crossValue).test)
    def mvnDeps = super.mvnDeps() ++ Seq(
      mvn"org.testcontainers:testcontainers:1.19.3",
      mvn"org.testcontainers:junit-jupiter:1.19.3",
      // Azure Cosmos Spark Connector - only for tests, not included in assembly
      // This is a shaded JAR so we keep it out of the main module dependencies
      mvn"com.azure.cosmos.spark:azure-cosmos-spark_3-5_2-12:4.42.0",
      // Force reactor-netty versions for runtime - same as main module
      mvn"io.projectreactor:reactor-core:3.6.11",
      mvn"io.projectreactor.netty:reactor-netty-core:1.2.13",
      mvn"io.projectreactor.netty:reactor-netty-http:1.2.13",
    ).map(_.forceVersion())
    override def testFramework = "org.scalatest.tools.Framework"
    def forkArgs = build.Constants.commonTestForkArgs ++ Seq(
      "-Dspark.sql.adaptive.enabled=false",
      "-Dspark.sql.adaptive.coalescePartitions.enabled=false",
      "-Dspark.serializer=org.apache.spark.serializer.KryoSerializer",
      "-Dspark.sql.hive.convertMetastoreParquet=false"
    )
  }

  def prependShellScript = ""
}
