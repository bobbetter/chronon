package build.cloud_azure
import mill.api._
import mill.scalalib._

// Cloud Azure module - supports both cloud_azure.test AND cloud_azure[2.13.17].test
object `package` extends Cross[CloudAzureModule](build.Constants.scalaVersions) with CloudAzureModule {
  // Provide default scala version for bracket-free access
  override val crossValue = build.Constants.defaultScalaVersion
}

trait CloudAzureModule extends Cross.Module[String] with build.BaseModule {
  def moduleDeps = Seq(build.spark(crossValue), build.aggregator(crossValue), build.api(crossValue), build.online(crossValue), build.redis(crossValue))

  // Versions force-overridden to fix CVEs from uber-jar transitive deps.
  // Also used by patchAssemblyMetadata to write correct pom.properties.
  private val jettyVersion = "12.0.12"
  private val avroVersion = "1.11.4"
  private val nimbusJoseJwtVersion = "9.37.4"

  def excludeJackson(dep: mill.scalalib.Dep): mill.scalalib.Dep = {
    dep
      .exclude("com.fasterxml.jackson.core" -> "jackson-core")
      .exclude("com.fasterxml.jackson.core" -> "jackson-databind")
      .exclude("com.fasterxml.jackson.core" -> "jackson-annotations")
      .exclude("com.fasterxml.jackson.datatype" -> "jackson-datatype-jsr310")
      .exclude("com.fasterxml.jackson.module" -> "jackson-module-scala_2.12")
      .exclude("com.fasterxml.jackson.module" -> "jackson-module-scala_2.13")
  }

  def mvnDeps = build.Constants.commonDeps ++ build.Constants.loggingApiDeps ++ build.Constants.utilityDeps ++ build.Constants.sparkDeps
    .map(_
      .exclude("org.apache.hive" -> "hive-exec")
      .exclude("org.apache.hive" -> "hive-llap-common")
      .exclude("org.apache.hive" -> "hive-llap-client")
      .exclude("org.apache.zookeeper" -> "zookeeper")
      .exclude("org.apache.zookeeper" -> "zookeeper-jute")
      .exclude("com.google.protobuf" -> "protobuf-java")
    ) ++ Seq(
    // Azure Identity for authentication
    mvn"com.azure:azure-identity:1.14.2"
      .exclude("com.azure" -> "azure-core-http-netty"),
    // Azure Key Vault for secret management (e.g., Snowflake private keys)
    mvn"com.azure:azure-security-keyvault-secrets:4.10.4"
      .exclude("com.azure" -> "azure-core-http-netty"),
    mvn"net.snowflake:snowflake-jdbc:3.16.0",
    mvn"net.snowflake:spark-snowflake_2.12:3.1.6",
    // BouncyCastle for parsing PEM private keys (required by Snowflake JDBC for key pair auth)
    mvn"org.bouncycastle:bcprov-jdk18on:1.80",
    mvn"org.bouncycastle:bcpkix-jdk18on:1.80",
    // Hadoop Azure FileSystem connector (for abfss:// support)
    // Use iceberg-azure (non-bundle) to avoid the bundle's partially-shaded reactor.netty
    // which conflicts with Cosmos's reactor-netty. Add azure-storage-file-datalake explicitly
    // since iceberg-azure marks it as provided/optional.
    mvn"org.apache.iceberg:iceberg-azure:1.10.0"
      .exclude("com.azure" -> "azure-core-http-netty"),
    mvn"com.azure:azure-storage-file-datalake:12.22.0"
      .exclude("com.azure" -> "azure-core-http-netty"),

    // Azure Cosmos DB SDK
    mvn"com.azure:azure-cosmos:4.76.0"
      .exclude("com.azure" -> "azure-core-http-netty"),
    // Use OkHttp for the azure-core HTTP pipeline (ADLSFileIO, azure-identity, keyvault).
    // Cosmos's internal ReactorNettyClient still uses reactor-netty/io.netty directly.
    mvn"com.azure:azure-core-http-okhttp:1.12.7",
    // Azure Cosmos DB Spark Connector
    // mvn"com.azure.cosmos.spark:azure-cosmos-spark_3-5_2-12:4.42.0",
    // CLI argument parsing for Spark loader
    mvn"org.rogach::scallop:5.0.0",
  ).map(excludeJackson) ++ Seq(
    mvn"com.fasterxml.jackson.core:jackson-core:2.17.2",
    mvn"com.fasterxml.jackson.core:jackson-databind:2.17.2",
    mvn"com.fasterxml.jackson.core:jackson-annotations:2.17.2",
    mvn"com.fasterxml.jackson.module::jackson-module-scala:2.17.2",
    mvn"com.fasterxml.jackson.datatype:jackson-datatype-jsr310:2.17.2",
    // Reactor and Netty dependencies required by Cosmos SDK's internal ReactorNettyClient
    // (used even in Gateway mode). Force versions compatible with azure-cosmos 4.76.0.
    mvn"io.netty:netty-all:4.1.118.Final",
    mvn"io.projectreactor:reactor-core:3.6.11",
    mvn"io.projectreactor.netty:reactor-netty-core:1.2.13",
    mvn"io.projectreactor.netty:reactor-netty-http:1.2.13",
    mvn"org.apache.hadoop:hadoop-client-runtime:3.4.1",
    mvn"org.apache.hadoop:hadoop-client-api:3.4.1",
    mvn"com.nimbusds:nimbus-jose-jwt:$nimbusJoseJwtVersion",
    mvn"org.eclipse.jetty:jetty-server:$jettyVersion",
    mvn"org.eclipse.jetty:jetty-http:$jettyVersion",
    mvn"org.eclipse.jetty:jetty-xml:$jettyVersion",
    mvn"org.apache.avro:avro:$avroVersion",
    mvn"net.minidev:json-smart:2.5.2",
  ).map(_.forceVersion())

  override def assemblyRules = super.assemblyRules ++ Seq(
    Assembly.Rule.Exclude("META-INF/versions/17/reactor/netty/resources/DefaultLoopNIO.class"),
    Assembly.Rule.Relocate("io.netty.**", "shaded.io.netty.@1"),
    // Shade OkHttp/Okio to avoid conflicts with Spark's bundled OkHttp 3.x
    // (azure-core-http-okhttp requires OkHttp 4.x for RequestBody.create(byte[]))
    Assembly.Rule.Relocate("okhttp3.**", "shaded.okhttp3.@1"),
    Assembly.Rule.Relocate("okio.**", "shaded.okio.@1"),
    // Uber-jars (spark-core, hadoop-client-runtime, iceberg-spark-runtime) embed
    // stale pom.properties that override the correct versions during assembly merge.
    // Exclude all conflicting metadata here; the assembly task patches the correct
    // versions back in from our resolved dependencies.
    Assembly.Rule.ExcludePattern("META-INF/maven/org\\.eclipse\\.jetty/.*"),
    Assembly.Rule.Exclude("org/sparkproject/jetty/version/build.properties"),
    Assembly.Rule.ExcludePattern("META-INF/maven/commons-beanutils/.*"),
    Assembly.Rule.ExcludePattern("META-INF/maven/org\\.apache\\.avro/.*"),
    Assembly.Rule.ExcludePattern("META-INF/maven/com\\.nimbusds/nimbus-jose-jwt/.*"),
  )

  // Uber-jars (spark-core, hadoop-client-runtime, iceberg-spark-runtime) embed stale
  // pom.properties that win the assembly merge. assemblyRules strips all copies;
  // this method patches the correct versions back so grype/SBOM tools see the actual
  // versions on the classpath. Versions are derived from the constants above.
  private val patchedMetadata: Seq[(String, String, String)] = Seq(
    ("org.eclipse.jetty", "jetty-server", jettyVersion),
    ("org.eclipse.jetty", "jetty-http", jettyVersion),
    ("org.apache.avro", "avro", avroVersion),
    ("com.nimbusds", "nimbus-jose-jwt", nimbusJoseJwtVersion),
  )

  private def patchAssemblyMetadata(jar: os.Path): Unit = {
    val uri = java.net.URI.create("jar:" + jar.toIO.toURI)
    val env = new java.util.HashMap[String, String]()
    val fs = java.nio.file.FileSystems.newFileSystem(uri, env)
    try {
      patchedMetadata.foreach { case (groupId, artifactId, version) =>
        val entry = fs.getPath(s"META-INF/maven/$groupId/$artifactId/pom.properties")
        java.nio.file.Files.createDirectories(entry.getParent)
        val content = s"artifactId=$artifactId\ngroupId=$groupId\nversion=$version\n"
        java.nio.file.Files.write(entry, content.getBytes(java.nio.charset.StandardCharsets.UTF_8))
      }
    } finally {
      fs.close()
    }
  }

  override def assembly = Task {
    val result = super.assembly()
    val out = Task.dest / "out.jar"
    os.copy(result.path, out)
    patchAssemblyMetadata(out)
    PathRef(out)
  }

  object test extends build.BaseTestModule {
    def scalaVersion = crossValue

    def moduleDeps = Seq(build.cloud_azure(crossValue), build.api(crossValue), build.online(crossValue), build.spark(crossValue).test, build.redis(crossValue).test)
    def mvnDeps = super.mvnDeps() ++ Seq(
      mvn"org.testcontainers:testcontainers:2.0.2",
      // Azure Cosmos Spark Connector - only for tests, not included in assembly
      // This is a shaded JAR so we keep it out of the main module dependencies
      // Exclude Jackson to avoid version conflicts with Spark's jackson-module-scala 2.15.2
      excludeJackson(mvn"com.azure.cosmos.spark:azure-cosmos-spark_3-5_2-12:4.42.0"),
      // Force Jackson 2.17.2 to match azure-cosmos SDK requirements
      // Also force jackson-module-scala to 2.17.2 for compatibility
      mvn"com.fasterxml.jackson.core:jackson-core:2.17.2",
      mvn"com.fasterxml.jackson.core:jackson-databind:2.17.2",
      mvn"com.fasterxml.jackson.core:jackson-annotations:2.17.2",
      mvn"com.fasterxml.jackson.module::jackson-module-scala:2.17.2",
    ).map(_.forceVersion())
    override def testFramework = "org.scalatest.tools.Framework"
    def forkArgs = build.Constants.commonTestForkArgs ++ Seq(
      "-Dspark.sql.adaptive.enabled=false",
      "-Dspark.sql.adaptive.coalescePartitions.enabled=false",
      "-Dspark.serializer=org.apache.spark.serializer.KryoSerializer",
      "-Dspark.sql.hive.convertMetastoreParquet=false"
    )
  }

  def prependShellScript = ""
}
